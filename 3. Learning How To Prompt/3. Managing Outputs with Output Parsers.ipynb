{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6ea013",
   "metadata": {},
   "source": [
    "- the importance of structured data output from language models, particularly in scenarios where the generated text needs to be used in applications or systems. It uses an example of a thesaurus application where the goal is to generate a list of substitute words for a given term, such as \"behavior.\" The issue arises because the generated text might vary in format, making it challenging to extract the relevant information programmatically.\n",
    "- \"Output Parsers,\" which are tools that help define and extract specific information from the generated text. These parsers create a structured data format that precisely outlines the expected output. In the case of the thesaurus application, the parser can extract the list of substitute words, even if the response format changes.\n",
    "\n",
    "# 1. Output Parsers\n",
    "Pydrantic parser is the most powerful and flexible wrapper, knowing the other options for less complicated problems is beneficial.\n",
    "\n",
    "## 1-1. PydanticOutputParser\n",
    "- instructs the model to generate its output in a JSON format and then extract the information from the response\n",
    "- be able to treat the parser’s output as a list, meaning it will be possible to index through the results without worrying about formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d99d8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.19) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Suggestions(BaseModel):\n",
    "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
    "\n",
    "    # Throw error in case of receiving a numbered-list from API\n",
    "    @validator('words')\n",
    "    def not_start_with_number(cls, field):\n",
    "        for item in field:\n",
    "            if item[0].isnumeric():\n",
    "                raise ValueError(\"The word can not start with numbers!\")\n",
    "        return field\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Suggestions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091dd59",
   "metadata": {},
   "source": [
    "We always import and follow the necessary libraries by creating the Suggestions schema class. There are two essential parts to this class:\n",
    "\n",
    "Expected Outputs: Each output is defined by declaring a variable with desired type, like a list of strings `(: List[str])` in the sample code, or it could be a single string `(: str)` if you are expecting just one word/sentence as the response. Also, It is required to write a simple explanation using the `Field` function’s `description` attribute to help the model during inference. (We will see an example of having multiple outputs later in the lesson)<br><br>\n",
    "Validators: It is possible to declare functions to validate the formatting. We ensure that the first character is not a number in the sample code. The function’s name is unimportant, but the `@validator` decorator must receive the same name as the variable you want to approve. (like `@validator(’words’)`) It is worth noting that the `field` variable inside the validator function will be a list if you specify it as one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98967511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Offer a list of suggestions to substitue the specified target_word based the presented context.\n",
    "{format_instructions}\n",
    "target_word={target_word}\n",
    "context={context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"target_word\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "model_input = prompt.format_prompt(\n",
    "\t\t\ttarget_word=\"behaviour\",\n",
    "\t\t\tcontext=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc5f08e",
   "metadata": {},
   "source": [
    "As discussed in previous lessons, the template variable is a string that can have named index placeholders using the following {variable_name} format. The template outlines our expectations for the model, including the expected formatting from the parser and the inputs. The PromptTemplate receives the template string with the details of each placeholder’s type. They could either be 1) input_variables whose value is initialized later on using the .format_prompt() function, or 2) partial_variables to be initialized instantly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b66098b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['conduct', 'manner', 'action', 'demeanor', 'comportment', 'deportment'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "model = OpenAI(model_name='text-davinci-003', temperature=0.0)\n",
    "\n",
    "output = model(model_input.to_string())\n",
    "\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a3e806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f73dc0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Suggestions(BaseModel):\n",
    "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
    "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
    "    \n",
    "    @validator('words')\n",
    "    def not_start_with_number(cls, field):\n",
    "        for item in field:\n",
    "            if item[0].isnumeric():\n",
    "                raise ValueError(\"The word can not start with numbers!\")\n",
    "                return field\n",
    "    \n",
    "    @validator('reasons')\n",
    "    def end_with_dot(cls, field):\n",
    "        for idx, item in enumerate( field ):\n",
    "            if item[-1] != \".\":\n",
    "                field[idx] += \".\"\n",
    "                return field\n",
    "parser = PydanticOutputParser(pydantic_object=Suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71ab9b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Offer a list of suggestions to substitute the specified target_word based on the presented context and the reasoning for each word.\n",
    "{format_instructions}\n",
    "target_word={target_word}\n",
    "context={context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"target_word\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "model_input = prompt.format_prompt(\n",
    "\t\t\ttarget_word=\"behaviour\",\n",
    "\t\t\tcontext=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62765f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=None, reasons=['refers to the way someone acts in a particular situation.', 'refers to the way someone behaves in a particular situation', 'refers to the way someone behaves in a particular situation', 'refers to the way someone behaves in a particular situation'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "model = OpenAI(model_name='text-davinci-003', temperature=0.0)\n",
    "\n",
    "output = model(model_input.to_string())\n",
    "\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc691a",
   "metadata": {},
   "source": [
    "# 1-2. CommaSeparatedOutputParser\n",
    "It is evident from the name of this class that it manages comma-separated outputs. It handles one specific case: anytime you want to receive a list of outputs from the model. Let’s start by importing the necessary modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7481f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59cb091f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Conduct',\n",
       " 'Actions',\n",
       " 'Demeanor',\n",
       " 'Mannerisms',\n",
       " 'Attitude',\n",
       " 'Performance',\n",
       " 'Reactions',\n",
       " 'Interactions',\n",
       " 'Habits',\n",
       " 'Repertoire',\n",
       " 'Disposition',\n",
       " 'Bearing',\n",
       " 'Posture',\n",
       " 'Deportment',\n",
       " 'Comportment']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The parser does not require a setting up step. Therefore it is less flexible. We can create the object by calling the class.\n",
    "The rest of the process for writing the prompt, initializing the model, and parsing the output is as follows.\"\"\"\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Prepare the Prompt\n",
    "template = \"\"\"\n",
    "Offer a list of suggestions to substitute the word '{target_word}' based the presented the following text: {context}.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"target_word\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "model_input = prompt.format(\n",
    "  target_word=\"behaviour\",\n",
    "  context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\"\n",
    ")\n",
    "\n",
    "# Loading OpenAI API\n",
    "model = OpenAI(model_name='text-davinci-003', temperature=0.0)\n",
    "\n",
    "# Send the Request\n",
    "output = model(model_input)\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b565f5",
   "metadata": {},
   "source": [
    "Although most of the sample code has been explained in the previous subsection, two parts might need attention. Firstly, we tried a new format for the prompt’s template to show different ways to write a prompt. Secondly, the use of .format() instead of .format_prompt() to generate the model’s input. The main difference compared to the previous subsection’s code is that we no longer need to call the .to_string() object since the prompt is already in string type.\n",
    "\n",
    "As you can see, the final output is a list of words that has some overlaps with the PydanticOutputParser approach with more variety. However, requesting additional reasoning information using the CommaSeparatedOutputParser class is impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa1884",
   "metadata": {},
   "source": [
    "# 1-3. StructuredOutputParser\n",
    "This is the first output parser implemented by the LangChain team. While it can process multiple outputs, it only supports texts and does not provide options for other data types, such as lists or integers. It can be used when you want to receive one response from the model. For example, only one substitute word in the thesaurus application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d2f5970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"words\", description=\"A substitue word based on context\"),\n",
    "    ResponseSchema(name=\"reasons\", description=\"the reasoning of why this word fits the context.\")\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5775f05",
   "metadata": {},
   "source": [
    "The above code demonstrates how to define a schema. However, we are not going to go into details. This class has no advantage since the PydanticOutputParser class provides validation and more flexibility for more complex tasks, and the CommaSeparatedOutputParser option covers more straightforward applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c5c94",
   "metadata": {},
   "source": [
    "# 2. Fixing Errors\n",
    "The parsers are powerful tools to dynamically extract the information from the prompt and validate it to some extent. Still, they do not guarantee a response. Imagine a situation where you deployed your application, and the model’s response [to a user’s request] is incomplete, causing the parser to throw an error. It is not ideal! In the following subsections, we will introduce two classes acting as fail-safe. They add a layer on top of the model’s response to help fix the errors.\n",
    "\n",
    "## 2-1. OutputFixingParser\n",
    "This method tries to fix the parsing error by looking at the model’s response and the previous parser. It uses a Large Language Model (LLM) to solve the issue. We will use GPT-3 to be consistent with the rest of the lesson, but it is possible to pass any supported model. Let’s start by defining the Pydantic data schema and show a sample error that could occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8304efc",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\output_parsers\\pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mjson_object\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\main.cp39-win_amd64.pyd\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\main.cp39-win_amd64.pyd\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15096/3458069529.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmissformatted_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'{\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissformatted_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\output_parsers\\pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"Failed to parse {name} from completion {text}. Got: {e}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mOutputParserException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_format_instructions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Suggestions(BaseModel):\n",
    "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
    "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Suggestions)\n",
    "\n",
    "missformatted_output = '{\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}'\n",
    "\n",
    "parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae12bd7",
   "metadata": {},
   "source": [
    "As you can see in the error message, the parser correctly identified an error in our sample response (missformatted_output) since we used the word reasoning instead of the expected reasons key. The OutputFixingParser class could easily fix this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f98958aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['conduct', 'manner'], reasons=['refers to the way someone acts in a particular situation.', 'refers to the way someone behaves in a particular situation.'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "model = OpenAI(model_name='text-davinci-003', temperature=0.0)\n",
    "\n",
    "outputfixing_parser = OutputFixingParser.from_llm(parser=parser, llm=model)\n",
    "outputfixing_parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f1b80",
   "metadata": {},
   "source": [
    "The from_llm() function takes the old parser and a language model as input parameters. Then, It initializes a new parser for you that has the ability to fix output errors. In this case, it successfully identified the misnamed key and changed it to what we defined.\n",
    "\n",
    "However, fixing the issues using this class is not always possible. Here is an example of using OutputFixingParser class to resolve an error with a missing key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03ec6be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['conduct', 'manner'], reasons=[\"The word 'conduct' implies a certain behavior or action, while 'manner' implies a polite or respectful way of behaving.\"])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missformatted_output = '{\"words\": [\"conduct\", \"manner\"]}'\n",
    "\n",
    "outputfixing_parser = OutputFixingParser.from_llm(parser=parser, llm=model)\n",
    "\n",
    "outputfixing_parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcceb78",
   "metadata": {},
   "source": [
    "Looking at the output, it is evident that the model understood the key reasons missing from the response but didn’t have the context of the desired outcome. It created a list with one entry, while we expect one reason per word. This is why we sometimes need to use the RetryOutputParser class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e47ad79",
   "metadata": {},
   "source": [
    "# 2-2. RetryOutputParser\n",
    "In some cases, the parser needs access to both the output and the prompt to process the full context, as demonstrated in the previous section. We first need to define the mentioned variables. The following codes initialize the LLM model, parser, and prompt, which were explained in more detail earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f7be48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Define data structure.\n",
    "class Suggestions(BaseModel):\n",
    "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
    "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Suggestions)\n",
    "\n",
    "# Define prompt\n",
    "template = \"\"\"\n",
    "Offer a list of suggestions to substitue the specified target_word based the presented context and the reasoning for each word.\n",
    "{format_instructions}\n",
    "target_word={target_word}\n",
    "context={context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"target_word\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "model_input = prompt.format_prompt(target_word=\"behaviour\", context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\")\n",
    "\n",
    "# Define Model\n",
    "model = OpenAI(model_name='text-davinci-003', temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4faad76",
   "metadata": {},
   "source": [
    "Now, we can fix the same missformatted_output using the RetryWithErrorOutputParser class. It receives the old parser and a model to declare the new parser object, as we saw in the previous section. However, the parse_with_prompt function is responsible for fixing the parsing issue while requiring the output and the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfaaf6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['conduct', 'manner'], reasons=[\"The word 'behaviour' can be substituted with 'conduct' to emphasize the students' actions in the classroom, and 'manner' to emphasize the students' attitude.\"])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import RetryWithErrorOutputParser\n",
    "\n",
    "missformatted_output = '{\"words\": [\"conduct\", \"manner\"]}'\n",
    "\n",
    "retry_parser = RetryWithErrorOutputParser.from_llm(parser=parser, llm=model)\n",
    "\n",
    "retry_parser.parse_with_prompt(missformatted_output, model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d83466",
   "metadata": {},
   "source": [
    "The outputs show that the RetryOuputParser has the ability to fix the issue where the OuputFixingParser was not able to. The parser correctly guided the model to generate one reason for each word.\n",
    "\n",
    "The best practice to incorporate these techniques in production is to catch the parsing error using a try: ... except: ... method. It means we can capture the errors in the except section and attempt to fix them using the mentioned classes. It will limit the number of API calls and avoid unnecessary costs that are associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70599401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
