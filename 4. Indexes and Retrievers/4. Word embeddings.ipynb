{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac53739c",
   "metadata": {},
   "source": [
    "**Embeddings** are **dense vector representations** of data that encapsulate semantic information, making them suitable for various machine-learning tasks such as **clustering, recommendation, and classification**. They transform human-perceived semantic similarity into closeness in vector space and can be generated for different data types, including text, images, and audio.\n",
    "\n",
    "For text data, models like the GPT family of models and Llama are **employed to create vector embeddings for words, sentences, or paragraphs**. In the case of **images, convolutional neural networks (CNNs) such as VGG and Inception can generate embeddings**. **Audio recordings** can be converted into vectors using image embedding techniques applied to visual representations of audio frequencies, like **spectrograms**. ```Deep neural networks are commonly employed to train models that convert objects into vectors. The resulting embeddings are typically high-dimensional and dense.```\n",
    "\n",
    "Embeddings are extensively used in **similarity search applications**, such as KNN and ANN, which **require calculating distances between vectors to determine similarity**. Nearest neighbor search can be employed for tasks like de-duplication, recommendations, anomaly detection, and reverse image search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89775fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"The cat is on the mat.\",\n",
    "    \"There is a cat on the mat.\",\n",
    "    \"The dog is in the yard.\",\n",
    "    \"There is a dog in the yard.\",\n",
    "]\n",
    "\n",
    "# Initialize the OpenAIEmbeddings instance\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Generate embeddings for the documents\n",
    "document_embeddings = embeddings.embed_documents(documents)\n",
    "\n",
    "# Perform a similarity search for a given query\n",
    "query = \"A cat is sitting on a mat.\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Calculate similarity scores\n",
    "similarity_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "\n",
    "# Find the most similar document\n",
    "most_similar_index = np.argmax(similarity_scores)\n",
    "most_similar_document = documents[most_similar_index]\n",
    "\n",
    "print(f\"Most similar document to the query '{query}':\")\n",
    "print(most_similar_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749fa54",
   "metadata": {},
   "source": [
    "**Embedding Models**\n",
    "\n",
    "Embedding models are a type of machine learning model that convert discrete data into continuous vectors. In the context of natural language processing, these discrete data points can be words, sentences, or even entire documents. The generated vectors, also known as embeddings, are designed to capture the semantic meaning of the original data.\n",
    "\n",
    "For instance, words that are semantically similar (e.g., 'cat' and 'kitten') would have similar embeddings. These embeddings are dense, which means that they use many dimensions (often hundreds) to capture nuances in meaning.\n",
    "\n",
    "The primary benefit of embeddings is that they allow us to use mathematical operations to reason about semantic meaning. For example, we can calculate the cosine similarity between two embeddings to assess how semantically similar the corresponding words or documents are.\n",
    "\n",
    "We initialize our embedding model. For this task, we've chosen the pre-trained \"sentence-transformers/all-mpnet-base-v2\" model. This model is designed to transform sentences into embeddings - vectors that encapsulate the semantic meaning of the sentences. The model_kwargs parameter is used here to specify that we want our computations to be performed on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5019a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6690a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "documents = [\"Document 1\", \"Document 2\", \"Document 3\"]\n",
    "doc_embeddings = hf.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd80c12",
   "metadata": {},
   "source": [
    "Now that we have our model, we define a list of documents - these are the pieces of text that we want to convert into semantic embeddings.\n",
    "\n",
    "With our model and documents ready, we move on to generate the embeddings. We do this by calling the ``embed_documents`` method on our HuggingFaceEmbeddings instance, passing our list of documents as an argument. This method processes each document and returns a corresponding list of embeddings.\n",
    "\n",
    "These embeddings are now ready for any downstream tasks such as classification, clustering, or similarity analysis. They represent our original documents in a form that machines can understand and process, enabling us to perform complex semantic tasks.\n",
    "\n",
    "**Cohere embeddings**\n",
    "\n",
    "Cohere is dedicated to making its innovative multilingual language models accessible to all, thereby democratizing advanced NLP technologies worldwide. Their Multilingual Model, which maps text into a semantic vector space for better text similarity understanding, significantly enhances multilingual applications such as search operations. Unlike their English language model, the multilingual model uses dot product computations resulting in superior performance. \n",
    "\n",
    "These multilingual embeddings are represented in a 768-dimensional vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac6d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cade08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "\n",
    "# Initialize the CohereEmbeddings object\n",
    "cohere = CohereEmbeddings(\n",
    "\tmodel=\"embed-multilingual-v2.0\",\n",
    "\tcohere_api_key=\"SCPQrM3ggZiI9pxQrxFumCfUav0fjipK9ONYXqf7\"\n",
    ")\n",
    "\n",
    "# Define a list of texts\n",
    "texts = [\n",
    "    \"Hello from Cohere!\", \n",
    "    \"مرحبًا من كوهير!\", \n",
    "    \"Hallo von Cohere!\",  \n",
    "    \"Bonjour de Cohere!\", \n",
    "    \"¡Hola desde Cohere!\", \n",
    "    \"Olá do Cohere!\",  \n",
    "    \"Ciao da Cohere!\", \n",
    "    \"您好，来自 Cohere！\", \n",
    "    \"कोहेरे से नमस्ते!\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for the texts\n",
    "document_embeddings = cohere.embed_documents(texts)\n",
    "\n",
    "# Print the embeddings\n",
    "for text, embedding in zip(texts, document_embeddings):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Embedding: {embedding[:5]}\")  # print first 5 dimensions of each embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fecfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd368ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our documents\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\",\n",
    "    \"Lady Gaga was born in 28 March 1986\",\n",
    "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a4e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize embeddings model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"<YOUR-ACTIVELOOP-ORG-ID>\"\n",
    "my_activeloop_dataset_name = \"langchain_course_embeddings\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retriever from db\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e306ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# istantiate the llm wrapper\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "# create the question-answering chain\n",
    "qa_chain = RetrievalQA.from_llm(model, retriever=retriever)\n",
    "\n",
    "# ask a question to the chain\n",
    "qa_chain.run(\"When was Michael Jordan born?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de01dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
