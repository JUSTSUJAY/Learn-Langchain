{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b320b3",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- explore how large language models learn token distributions and predict the next token, allowing them to generate human-like text that can both amaze and perplex us\n",
    "## LLMs in general\n",
    "- deep learning models with billions of parameters that excel at a wide range of natural language processing tasks\n",
    "- perform tasks like translation, sentiment analysis, and chatbot conversations without being specifically trained for them.\n",
    "### Architecture\n",
    "- LLM = Neural Network+FeedForward Layers+Embedding Layers+attention layers\n",
    "### Maximum tokens\n",
    "- Maximum input tokens depend on the model we are ranging from few thousands to tens of thousands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62685fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name', 'is Tyler', 'Durden']\n"
     ]
    }
   ],
   "source": [
    "text = 'My name is Tyler Durden'\n",
    "max_tokens = 2\n",
    "def split_text(text,max_tokens):\n",
    "    i = 0\n",
    "    prompt = text.split()\n",
    "    to_be_returned = list()\n",
    "    while(i<len(prompt)):\n",
    "        to_be_returned.append(' '.join(prompt[i:i+max_tokens]))\n",
    "        i = i+n\n",
    "    return to_be_returned\n",
    "\n",
    "answer = split_text(text,max_tokens)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fb9dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we input number of tokens more than maximum tokens, \n",
    "# It causes the truncation and also some error too.\n",
    "# typically better models have high value of max number of tokens\n",
    "# in such cases we can use chop and feed approach\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model = 'text-davinci-003')\n",
    "input_text = 'Some long text of yours'\n",
    "\n",
    "# max tokens \n",
    "max_tokens = 1000\n",
    "\n",
    "text_chunks = split_text(input_text,max_tokens)\n",
    "\n",
    "# since we are feeding chunk by chunk, we will store the result chunk by chunk too\n",
    "results = list()\n",
    "for chunk in text_chunks:\n",
    "    pass\n",
    "#     results.append(llm(chunk)) uncomment at time of real execution\n",
    "    \n",
    "def combine_result(results):\n",
    "#     write some combine function to connect all those seperately produced results\n",
    "    pass\n",
    "\n",
    "final_result = combine_result(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6bbaac",
   "metadata": {},
   "source": [
    "# Tokens Distributions and Predicting the Next Token\n",
    "-  to predict the next token\n",
    "- Gpt family uses Casual Language Modeling(CLM) to predict the next token from the knowledge of previous ones\n",
    "- This help LLM to produce the relevant text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cad122a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rainbow Socks Co.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802e523f",
   "metadata": {},
   "source": [
    "Openai gives a credit of $5 api usage on a free account\n",
    "\n",
    "We can check how we are charger per usage of the api using callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef70a126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 72\n",
      "\tPrompt Tokens: 7\n",
      "\tCompletion Tokens: 65\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00144\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "llm = OpenAI(model_name = 'text-davinci-003',n = 2, best_of = 2)\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm('Tell me a LLM JOke')\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e33f9272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q: What did the LLM student say to the barista?\n",
      "A: \"Espresso me, please!\"\n"
     ]
    }
   ],
   "source": [
    "# lets see what was the joke\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ad912",
   "metadata": {},
   "source": [
    "# Few Shot Learning\n",
    "- ability of LLM to learn and generalize from limited text \n",
    "## Process to apply FSL\n",
    "- We use `FewShotPromptTemplate` class(its instance to be exact) which takes - `PromptTemplate` and few shot examples as input\n",
    "- Prompt templates passed will be formatted as few shot examples\n",
    "- helps the model generate better responses\n",
    "\n",
    "lets see how to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e417f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the weather like?\",\n",
    "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
    "    }, {\n",
    "        \"query\": \"How old are you?\",\n",
    "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create an example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"This text was generated by:\n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5da977ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The meaning of life is to find your own meaning and purpose.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "chain.run(\"What's the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39f382c",
   "metadata": {},
   "source": [
    "# Emergent Abilities of LLMs:\n",
    "\n",
    "- LLMs possess emergent abilities due to extensive pre-training on large datasets.\n",
    "- These abilities are not explicitly programmed but emerge as the model learns patterns.\n",
    "- LangChain models leverage these emergent abilities for various tasks using different model types.\n",
    "- LLMs can perform tasks like answering questions, generating text, and making recommendations.\n",
    "\n",
    "# Scaling Laws and Performance:\n",
    "\n",
    "- Scaling laws describe the relationship between model size, training data, and performance.\n",
    "- Generally, larger models with more training data tend to perform better.\n",
    "- Performance improvement follows diminishing returns, not always linearly.\n",
    "- Balance between model size, training data, performance, and resources is crucial.\n",
    "\n",
    "# Limitations of LLMs:\n",
    "\n",
    "- LLMs have notable capabilities but also limitations.\n",
    "- Hallucinations can occur, producing text that seems plausible but is factually incorrect.\n",
    "- Biases from training data can lead to outputs that perpetuate stereotypes or yield undesired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af648963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q huggingface_hub\n",
    "# install hf hub if you haven't already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25498601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a short and simple question and answer template\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"What is the capital city of France?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7024d0",
   "metadata": {},
   "source": [
    "- Now we will use flan-t5-large model from hugging face hub - since we have provided the api key in our environmentvariables it will load the model through api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "021bc88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "import os\n",
    "huggingfacehub_api_token = os.environ.get('HUGGINGFACEHUB_API_TOKEN')\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(huggingfacehub_api_token = huggingfacehub_api_token,\n",
    "        repo_id='google/flan-t5-large',\n",
    "    model_kwargs={'temperature':0}\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "# ask the user question about the capital of France\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef873a6",
   "metadata": {},
   "source": [
    "Lets modify our prompt to let it answer multiple questions\n",
    "- We have two approaches to let it answer\n",
    "\n",
    "- **Approach 1 - Feed one by one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37c31260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='paris', generation_info=None)], [Generation(text='giraffe', generation_info=None)], [Generation(text='nitrogen', generation_info=None)], [Generation(text='yellow', generation_info=None)]] llm_output=None run=[RunInfo(run_id=UUID('09876aa8-0ffa-4100-b74a-4b37bcb65da6')), RunInfo(run_id=UUID('da9e6756-3ac0-46c3-983c-6da7ed9d71cb')), RunInfo(run_id=UUID('eec65899-5ca1-4167-83b6-ddb83dd8a736')), RunInfo(run_id=UUID('d5034348-f6f8-4e95-a080-e39505ebd118'))]\n"
     ]
    }
   ],
   "source": [
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]\n",
    "res = llm_chain.generate(qa)\n",
    "print( res )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a3a8f",
   "metadata": {},
   "source": [
    "- **Approach 2 - feed all at once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec3e2320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris\\nBlue Whale\\nNitrogen\\nYellow'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"What is the capital city of France?\\n\" +\n",
    "    \"What is the largest mammal on Earth?\\n\" +\n",
    "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
    "    \"What color is a ripe banana?\\n\"\n",
    ")\n",
    "llm_chain.run(qs_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95e435",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c18cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_template = 'summarize the following text in one sentence: {text}'\n",
    "summarize_prompt = PromptTemplate(template = summarize_template,input_variables = ['text'])\n",
    "summarize_chain = LLMChain(llm = llm,prompt=summarize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74a78655",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\n",
    "summarized_text = summarize_chain.predict(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ec0a1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain provides modules that can be used both individually and in combination to create language model applications, with the most basic building block being the calling of an LLM on some input.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daade2b",
   "metadata": {},
   "source": [
    "# Text Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27411d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
    "translation_prompt = PromptTemplate(input_variables=[\"source_language\", \"target_language\", \"text\"], template=translation_template)\n",
    "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aaa2da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = \"English\"\n",
    "target_language = \"Spanish\"\n",
    "text = \"Sujay Will be new Andrej Kerpathy\"\n",
    "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ca1546d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sujay será el nuevo Andrej Kerpathy.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6458563c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
