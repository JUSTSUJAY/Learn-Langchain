{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d79bda",
   "metadata": {},
   "source": [
    "will examine the differences between LLMs and Chat Models, their unique use cases, and how they are implemented within LangChain.\n",
    "\n",
    "# Large Language Models (LLMs)\n",
    "\n",
    "LLMs, such as GPT-3, Bloom, PaLM, and Aurora genAI, are capable of taking a text string as input and producing a text string as output. They undergo training on language modeling tasks, enabling them to generate human-like text, perform intricate reasoning, and even generate code. LLMs offer versatility and power, making them suitable for a wide array of tasks. However, they may occasionally provide incorrect or nonsensical answers, and their API is less structured compared to Chat Models.\n",
    "\n",
    "## Pre-training and Learning Process\n",
    "\n",
    "LLMs are pre-trained by exposing them to massive text corpora and having them predict the next word. This process helps them understand word relationships and generate high-quality text. This ability finds applications in various areas, including predictive text on smartphones and automatic form-filling.\n",
    "\n",
    "## General vs. Domain-Specific Training\n",
    "\n",
    "Most LLMs are trained on general-purpose datasets, while some incorporate both general and domain-specific data. For instance, Intel Aurora genAI is trained on general text, scientific data, and codes from its domain. Domain-specific LLMs aim to excel in a particular field while maintaining the capability to handle general tasks.\n",
    "\n",
    "## Expanding Influence and Integration\n",
    "\n",
    "LLMs are poised to impact diverse realms of human life, spanning arts, sciences, and law. As they continue to evolve, they will become integral to education, personal activities, and professional work, solidifying their status as essential technologies to master.\n",
    "\n",
    "## Using LLMs in LangChain\n",
    "\n",
    "To use a large language model like GPT-3 in LangChain, follow these steps:\n",
    "1. Import the `OpenAI` wrapper from `langchain.llms` module.\n",
    "2. Initialize the wrapper with the desired model name and any extra arguments, such as adjusting the temperature for varied outputs.\n",
    "3. Create a `PromptTemplate` to format input for the model.\n",
    "4. Define an `LLMChain` to combine the model and the prompt.\n",
    "5. Execute the chain with your desired input using `.run()`.\n",
    "\n",
    "By following these steps, you can seamlessly incorporate the power of LLMs into your LangChain projects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a8112cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Andrej Karpathy is a computer scientist and artificial intelligence researcher, best known for his work on deep learning and computer vision. He is currently a research scientist at OpenAI, a research laboratory focused on artificial intelligence. He is also an adjunct professor at Stanford University, where he teaches a course on deep learning for computer vision.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=[\"person\"],\n",
    "  template=\"Who is {person} related to machine learning?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print( chain.run(\"Andrej Kerpathy\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0bb28d",
   "metadata": {},
   "source": [
    "# Chat Models in LangChain\n",
    "\n",
    "Chat Models are a prominent feature in LangChain, exemplified by ChatGPT, which can utilize GPT-3 or GPT-4 as its core. These models have garnered attention due to their capacity to learn from human feedback and their user-friendly chat interface.\n",
    "\n",
    "## Functionality of Chat Models\n",
    "\n",
    "Chat Models, like ChatGPT, take a list of messages as input and return an `AIMessage`. While they often employ LLMs at their core, they have more structured APIs. Designed to remember past interactions within a session, Chat Models use context to generate contextually relevant responses. They leverage reinforcement learning from human feedback to enhance their responses. Despite their strengths, limitations in reasoning persist, necessitating careful handling to prevent inappropriate content generation.\n",
    "\n",
    "## API Differences and Message Types\n",
    "\n",
    "### SystemMessage\n",
    "System Messages offer initial instructions, context, or data for the AI model. They establish objectives and guide the AI's behavior, ensuring specific outcomes.\n",
    "\n",
    "### HumanMessage\n",
    "Human Messages are user inputs, representing their interactions with the AI model. The model responds to these inputs, aiming to address user queries effectively. LangChain allows customization of the human prefix in conversation summaries.\n",
    "\n",
    "### AIMessage\n",
    "AI Messages convey the AI's responses from its perspective in interactions with users. They reflect the model's replies to human inputs. Similar to Human Messages, LangChain permits customization of the AI prefix in conversation summaries.\n",
    "\n",
    "## Using ChatOpenAI with HumanMessage\n",
    "\n",
    "As an example, we're creating a chatbot for translating English to French using the LangChain library. This extends beyond the previous lesson's scope, involving multiple message types for better communication. This approach enhances the model's understanding of user requirements.\n",
    "\n",
    "- Create a list of messages, starting with a SystemMessage setting the context as an English-to-French translation assistant.\n",
    "- Follow it with a HumanMessage containing the user's English sentence for translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ed5504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Me encanta programar.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "  HumanMessage,\n",
    "  SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "messages = [\n",
    "\tSystemMessage(content=\"You are a helpful assistant that translates English to Spanish.\"),\n",
    "\tHumanMessage(content=\"Translate the following sentence: I love programming.\")\n",
    "]\n",
    "\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d104d0",
   "metadata": {},
   "source": [
    "As you can see, we pass the list of messages to the chatbot using the `chat()` function. The chatbot processes the input messages, considers the context provided by the system message, and then translates the given English sentence into French.\n",
    "\n",
    "`SystemMessage` represents the messages generated by the system that wants to use the model, which could include instructions, notifications, or error messages. These messages are not generated by the human user or the AI chatbot but are instead produced by the underlying system to provide context, guidance, or status updates.\n",
    "\n",
    "Using the generate method, you can also generate completions for multiple sets of messages. Each batch of messages can have its own `SystemMessage` and will perform independently. The following code shows the first set of messages translate the sentences from English to French, while the second ones do the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f188eb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[ChatGeneration(text='Me encanta programar.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='Me encanta programar.', additional_kwargs={}, example=False))], [ChatGeneration(text='I love programming.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='I love programming.', additional_kwargs={}, example=False))]] llm_output={'token_usage': {'prompt_tokens': 64, 'completion_tokens': 10, 'total_tokens': 74}, 'model_name': 'gpt-3.5-turbo'} run=[RunInfo(run_id=UUID('1cec7ae0-306b-45fa-b0d5-54be564cdead')), RunInfo(run_id=UUID('64bf8cbd-cd74-4f73-96a4-a49083bff825'))]\n"
     ]
    }
   ],
   "source": [
    "batch_messages = [\n",
    "  [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to Spanish.\"),\n",
    "    HumanMessage(content=\"Translate the following sentence: I love programming.\")\n",
    "  ],\n",
    "  [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates French to English.\"),\n",
    "    HumanMessage(content=\"Translate the following sentence: Me encanta programar.\")\n",
    "  ],\n",
    "]\n",
    "print( chat.generate(batch_messages) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f875a06",
   "metadata": {},
   "source": [
    ">llm_output:  {'product': 'Translate the following text from English to French: Hello, how are you?', 'text': '\\n\\nBonjour, comment allez-vous?'}\n",
    "\n",
    ">chat_output:  content='Bonjour, comment ça va ?' additional_kwargs={} example=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da945d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
