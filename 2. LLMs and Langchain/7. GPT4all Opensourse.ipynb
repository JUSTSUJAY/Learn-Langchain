{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d02b91c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "514266it [5:06:50, 27.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# local_path = 'E:\\ML\\llama.cpp\\models\\gpt4all-lora-quantized-ggml.bin'\n",
    "# Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# url = 'https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized-ggml.bin'\n",
    "\n",
    "# # send a GET request to the URL to download the file.\n",
    "# response = requests.get(url, stream=True)\n",
    "\n",
    "# # open the file in binary mode and write the contents of the response\n",
    "# # to it in chunks.\n",
    "# with open(local_path, 'wb') as f:\n",
    "#     for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
    "#         if chunk:\n",
    "#             f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fa190a",
   "metadata": {},
   "source": [
    "The LangChain library uses PyLLaMAcpp module to load the converted GPT4All weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f5646",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = GPT4All(model=\"./models/ggml-model-q4_0.bin\", callback_manager=callback_manager, verbose=True)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a5ead1",
   "metadata": {},
   "source": [
    "The default behavior is to wait for the model to finish its inference process to print out its outputs. However, it could take more than an hour (depending on your hardware) to respond to one prompt because of the large number of parameters in the model. We can use the StreamingStdOutCallbackHandler() callback to instantly show the latest generated token. This way, we can be sure that the generation process is running and the model shows the expected behavior. Otherwise, it is possible to stop the inference and adjust the prompt.\n",
    "\n",
    "The GPT4All class is responsible for reading and initializing the weights file and setting the required callbacks. Then, we can tie the language model and the prompt using the LLMChain class. It will enable us to ask questions from the model using the run() object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe762038",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What happens when it rains somewhere?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da0425",
   "metadata": {},
   "outputs": [],
   "source": [
    "funny_template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's answer in two sentence while being funny.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=funny_template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2755b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d081ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851bb283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
