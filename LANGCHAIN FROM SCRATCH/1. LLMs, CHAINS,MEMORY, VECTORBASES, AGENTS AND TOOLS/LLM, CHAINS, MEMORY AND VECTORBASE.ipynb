{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2740055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keys import Openaikey,Deeplakekey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da86eb",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "The fundamental component of LangChain involves invoking an LLM with a specific input. To illustrate this, we'll explore a simple example. Let's imagine we are building a service that suggests personalized workout routines based on an individual's fitness goals and preferences.<br>\n",
    "To accomplish this, we will first need to import the LLM wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe1eb12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==0.0.208 deeplake openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e834ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b65d3",
   "metadata": {},
   "source": [
    "- Tempreture parameter manages the randomness of the output\n",
    "- temp == 0 -> predetermined outputs and used for most probable results\n",
    "- temp == 1 -> inconsistent and interesting tasks - not advised to use it\n",
    "- prefered 0.7 - 0.9\n",
    "- use gpt3 variant Davinci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68669e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model = 'text-davinci-003',temperature = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e17a0a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'What is the capital of India?'\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b750ab7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-UzOocMsB0zH8LN0hOTaQ84JX on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-UzOocMsB0zH8LN0hOTaQ84JX on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-UzOocMsB0zH8LN0hOTaQ84JX on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-UzOocMsB0zH8LN0hOTaQ84JX on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-Monday: Begin with a 10-minute warm-up jog. After, run an interval route of sprinting for 1 minute and jogging for 2 minutes, for a total of 20 minutes. End with a 5 minute cool-down jog.\n",
      "\n",
      "-Tuesday: Begin with a 10-minute warm-up jog. After, run hills for a total of 30 minutes, alternating between sprinting up the hill and jogging down. End with a 5 minute cool-down jog.\n",
      "\n",
      "-Wednesday: Take a day off.\n",
      "\n",
      "-Thursday: Begin with a 10-minute warm-up jog. After, do 3 sets of 10 burpees with a 30-second sprint in between each set. End with a 5-minute cool-down jog.\n",
      "\n",
      "-Friday: Begin with a 10-minute warm-up jog. After, go on a long distance run for a total of 45 minutes. End with a 5-minute cool-down jog.\n",
      "\n",
      "-Saturday: Take a day off. \n",
      "\n",
      "-Sunday: Begin with a 10-minute warm-up jog. After, go on a hike for a total of 1 hour. Make sure to find hills and up the intensity by running the hills. End with\n"
     ]
    }
   ],
   "source": [
    "text = 'Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities'\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d179a",
   "metadata": {},
   "source": [
    "## Chain\n",
    "- A chain is a wrapper around multiple individual components which are combined for a common task<br>\n",
    "common chain - LLMChain\n",
    "\n",
    "Work of llmchain\n",
    "- multiple inputs variables\n",
    "- Use `PromptTemplate` to format the input variables into the prompt\n",
    "- pass the formated prompt into the llm\n",
    "- used output parser to parse the output for the final output\n",
    "\n",
    "In the next example, we demonstrate how to create a chain that generates a possible name for a company that produces eco-friendly water bottles. By using LangChain's LLMChain, PromptTemplate, and OpenAIclasses, we can easily define our prompt, set the input variables, and generate creative outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc092037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a940a55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-UzOocMsB0zH8LN0hOTaQ84JX on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-UzOocMsB0zH8LN0hOTaQ84JX on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nEcoQuench Bottle Co.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(input_variables = ['Products'],\n",
    "                       template = 'What is a good name for a company that makes {Products}')\n",
    "chain = LLMChain(llm = llm, prompt = prompt)\n",
    "# chain uses run() function\n",
    "chain.run('Eco-friendly water bottles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32827495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(''.join(chain.run('Eco-friendly water bottle').strip().split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50e2dc",
   "metadata": {},
   "source": [
    "## Memory\n",
    "- Stores the interaction between user and the bot\n",
    "- helps maintain the context and coherency throughout the interaction\n",
    "- if it has memory of previous interactions then the output it produces in the future will be more releavant and accurate\n",
    "- `ConversationBufferMemory` acts as a wrapper around the `ChatMessageHistory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04e21390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.memory import ConversationBufferMemory\n",
    "# from langchain.chains import ConversationChain\n",
    "\n",
    "# conversation_chain = ConversationChain(llm = llm, verbose = True,memory = ConversationBufferMemory())\n",
    "\n",
    "# conversation_chain.predict(input = 'Tell me about yourself')\n",
    "# conversation_chain.predict(input = 'What can you do?')\n",
    "# conversation_chain.predict(input = 'Can you help me with my work?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacd975d",
   "metadata": {},
   "source": [
    "## Deep Lake vectorstore\n",
    "- Deep Lake provides storage for embeddings and their corresponding metadata in the context of LLM apps. It enables hybrid searches on these embeddings and their attributes for efficient data retrieva\n",
    "- It’s multimodal, which means that it can be used to store items of diverse modalities, such as texts, images, audio, and video, along with their vector representations\n",
    "- It’s serverless, which means that we can create and manage cloud datasets without creating and managing a database instance. This aspect gives a great speedup to new projects.\n",
    "- Last, it’s possible to easily create a data loader out of the data loaded into a Deep Lake dataset. It is convenient for fine-tuning machine learning models using common frameworks like PyTorch and TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "706bf83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = Deeplakekey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a84f6efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d063da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain import vectorstores\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98918954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\utils\\utils.py:155: UserWarning: WARNING! tempreture is not default parameter.\n",
      "                tempreture was transferred to model_kwargs.\n",
      "                Please confirm that tempreture is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ll = OpenAI(model = 'text-davinci-003',tempreture = 0)\n",
    "embeddings = OpenAIEmbeddings(model = 'text-embedding-ada-002')\n",
    "\n",
    "# create our documents\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaa6db58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://rayroyale33/langchain_course_from_zero_to_hero already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://rayroyale33/langchain_course_from_zero_to_hero', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (6, 1536)  float32   None   \n",
      "    id        text      (6, 1)      str     None   \n",
      " metadata     json      (6, 1)      str     None   \n",
      "   text       text      (6, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['e2129d0b-3b4e-11ee-b60b-a497b1b4542e',\n",
       " 'e212c405-3b4e-11ee-98a7-a497b1b4542e']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_activeloop_org_id = \"rayroyale33\" \n",
    "my_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = vectorstores.DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b8c2f",
   "metadata": {},
   "source": [
    "you’ve just created your first Deep Lake dataset!\n",
    "\n",
    "Now, let's create a RetrievalQA chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9ef53bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_qa = RetrievalQA.from_chain_type(llm = llm, \n",
    "                                          chain_type = 'stuff',\n",
    "                                          retriever = db.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147963af",
   "metadata": {},
   "source": [
    "Next, let's create an agent that uses the RetrievalQA chain as a tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d2f277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Retrieval QA System\",\n",
    "        func=retrieval_qa.run,\n",
    "        description=\"Useful for answering questions.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent = initialize_agent(tools,llm,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb959f8",
   "metadata": {},
   "source": [
    "Finally, we can use the agent to ask a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b525394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out when Napoleone was born\n",
      "Action: Retrieval QA System\n",
      "Action Input: When was Napoleone born?\n",
      " timetable\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m Napoleon Bonaparte was born on 15 August 1769.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Napoleon Bonaparte was born on 15 August 1769.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Napoleon Bonaparte was born on 15 August 1769.\n"
     ]
    }
   ],
   "source": [
    "response = agent.run('When was Napoleone born?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf748a",
   "metadata": {},
   "source": [
    "Here, the agent used the “Retrieval QA System” tool with the query “When was Napoleone born?” which is then run on our new Deep Lake dataset, returning the most similar document (i.e., the document containing the date of birth of Napoleon). This document is eventually used to generate the final output.\n",
    "\n",
    "Things that happened\n",
    "- We created a vector database\n",
    "- Created an agent with RetrievalQA chain as a tool to answer questions based on our document\n",
    "\n",
    "Next - Add more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1c8c54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://rayroyale33/langchain_course_from_zero_to_hero already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://rayroyale33/langchain_course_from_zero_to_hero', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (8, 1536)  float32   None   \n",
      "    id        text      (8, 1)      str     None   \n",
      " metadata     json      (8, 1)      str     None   \n",
      "   text       text      (8, 1)      str     None   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['05e4c6f3-3b4f-11ee-b8f9-a497b1b4542e',\n",
       " '05e4c6f4-3b4f-11ee-9aea-a497b1b4542e']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # load the existing Deep Lake dataset and specify the embedding function\n",
    "db = vectorstores.DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# create new documents\n",
    "texts = [\n",
    "    \"Lady Gaga was born in 28 March 1986\",\n",
    "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1e707ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the date of Michael Jordan's birth\n",
      "Action: Retrieval QA System\n",
      "Action Input: When was Michael Jordan born?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m Michael Jordan was born on 17 February 1963.\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-UzOocMsB0zH8LN0hOTaQ84JX on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Michael Jordan was born on 17 February 1963.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Michael Jordan was born on 17 February 1963.\n"
     ]
    }
   ],
   "source": [
    "# now that we have new information lets ask some new question\n",
    "response = agent.run(\"When was Michael Jordan born?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff1e90",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91187bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "920f94ff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
